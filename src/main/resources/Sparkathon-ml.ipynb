{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark ML/MLlib - Machine Learning with Spark for beginners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agenda\n",
    "\n",
    "1. About Spark (with projects)\n",
    "2. About these notebooks\n",
    "3. *RDD* vs *DataFrame* and *MLlib* vs *ML*\n",
    "4. ML overview\n",
    "5. Pipelines\n",
    "6. Supervised learning\n",
    "7. We need to go deeper!\n",
    "8. Where to go from there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test of environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"hello from Scala\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### about this notebooks\n",
    "\n",
    "This is a fork  of a popular [Docker](http://docker.com) image [\"All spark notebook\"](https://hub.docker.com/r/jupyter/all-spark-notebook/) by [Jupyter](http://jupyter.org/).\n",
    "\n",
    "I used image mentioned above as baseline and included this notebook and couple of datasets.\n",
    "\n",
    "### why notebooks?\n",
    "\n",
    "Notebooks lets you mix code and documentation (uses Markdown). Results can contain code, graphs of results, etc. I hope you also find notebooks convenient for easy to start, easy to reproduce environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark overview\n",
    "\n",
    "**Apache Spark** is general purpose engine for distributed data processing. The foundation of Spark is **Spark Core** — software layer that handles task dispatching, scheduling, IO operations. The center of Spark Core API is a concept of Resilient Distributed Dataset (**RDD**) — a collection that is distibuted across Spark cluster\n",
    "\n",
    "There are 4 officials project on top **Spark Core**:\n",
    "\n",
    "* **Spark SQL**: high level API of Spark that provides DataFrame and SQL abstraction over Dataset. You should always try to use high level API first.\n",
    "\n",
    "* **Spark (Structured) Streaming**: Provides interface for streaming. Structured streaming provides high level DataFrame interface. It ingests data in mini-batches and perfroms transformations on those data. High level interface leverages DataFrames to allows to reuse code for both streaming and batch processing, helping to create [lambda architecture](https://en.wikipedia.org/wiki/Lambda_architecture)\n",
    "\n",
    "* **Spark MLlib (ML)**: distributed machine learning framework, that provides:\n",
    "    * [summary statistics](https://en.wikipedia.org/wiki/Summary_statistics), [correlations](https://en.wikipedia.org/wiki/Correlation_and_dependence), [stratified sampling](https://en.wikipedia.org/wiki/Stratified_sampling), [hypothesis testing](https://en.wikipedia.org/wiki/Statistical_hypothesis_testing), random data generation\n",
    "    * [classification](https://en.wikipedia.org/wiki/Statistical_classification) and [regression](https://en.wikipedia.org/wiki/Regression_analysis): [support vector machines](https://en.wikipedia.org/wiki/Support_vector_machines), [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression), [linear regression](https://en.wikipedia.org/wiki/Linear_regression), decision trees, [naive Bayes classification](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n",
    "    * [collaborative filtering](https://en.wikipedia.org/wiki/Collaborative_filtering) techniques including alternating least squares (ALS)\n",
    "    * [cluster analysis](https://en.wikipedia.org/wiki/Cluster_analysis) methods including [k-means](https://en.wikipedia.org/wiki/K-means_clustering), and [Latent Dirichlet Allocation (LDA)](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)\n",
    "    * [dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction) techniques such as [singular value decomposition (SVD)](https://en.wikipedia.org/wiki/Singular-value_decomposition), and [principal component analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis)\n",
    "    * [feature extraction](https://en.wikipedia.org/wiki/Feature_extraction) and [transformation functions](https://en.wikipedia.org/wiki/Data_transformation_(statistics))\n",
    "    * optimization algorithms such as [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent), [limited-memory BFGS (L-BFGS)](https://en.wikipedia.org/wiki/Limited-memory_BFGS)\n",
    "\n",
    "\n",
    "\n",
    "![Apache Spark Architecture](https://spark.apache.org/images/spark-stack.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _RDD_ vs _DataFrame_ and _MLlib_ vs _ML_\n",
    "\n",
    "Spark has 2 API:\n",
    "* **Structured API** — high level API (includes DataFrame, Dataset, SparkSQL, Views and various interfaces for manipulation all sorts of data, from unstructured log files to semi-structured CSV to highly structured relation tables or Parquet files)\n",
    "* **Low-level API** — you should always favor high-level API, since it is well suited for most scenarios, however there are times where high-level manipulations will not meet business or engineering problem you are trying to solve. For those cases you can try to use Spark's low-level API, specifically RDD, SparkContext, distributed shared variables, like accumulators, broadcast variables and etc.\n",
    "\n",
    "Spark's Machine learning library have two implementations **RDD based** and **DataFrame Base**.\n",
    "As of Spark 2.0 RDD-based API is in maintenance mode, so you should check for new features in Spark Dataframe MLlib library. It often called Spark ML. (due to _org.apache.spark.ml_ package)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML overview\n",
    "\n",
    "Machine learning is set of techniques aimed at deriving insights and making predicitions or recommendationd based on dataset.\n",
    "Common task include:\n",
    "1. **Recomendation engines** — transform existing information about customer choices into suggestetions for new or existing customers\n",
    "2. **Unsupervised learning** — clustering, anomaly detection, topic modeling, etc — techniques for discovery structure of the data\n",
    "3. **Supervised learning** — classification and regression. Goal to predict a label for data point using features.\n",
    "\n",
    "On a very high-level typical machine learning process looks like that:\n",
    "\n",
    "![Machine learning process](https://i.imgur.com/0XAG2c0.png)\n",
    "\n",
    "\n",
    "MLlib is a package, built on and included in Spark, that provides interfaces for: gathering and cleaning data, feature engineering and feature selection, training and tuning large-scale supervised and unsupervised machine learning models, and using those models in production.\n",
    "\n",
    "### Why should anyone use Spark MLlib?\n",
    "\n",
    "Basically, you have a lot of ML frameworks, but not all of them scales as well as Spark MLlib.\n",
    "Spark is distributed first framework and leverages distributed collections to do heavy lifting for you — reduce amount of time needed for training a model and allow use datasets lager than 1 computer instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines\n",
    "\n",
    "Spark ML library utilizes Dataframes as data source via **ML Pipelines API**.\n",
    "Pipeline helps developer to create sequence of data transofmations.\n",
    "\n",
    "Main Pipeline concepts:\n",
    "* **Dataframe** — represent our data. Can hold a variety of data types (text, feature vectors, ground truth labels, predictions)\n",
    "* **Transformer** — algorithm that transforms Dataframe into another Dataframe.\n",
    "    Examples:\n",
    "    * Normalizer — it transforms raw data to normalized data\n",
    "    * ML model — transforms DataFrame to DataFrame with predictions\n",
    "    \n",
    "* **Estimator** — algoritm that can be fit on Dataframe to produce Transfromer. Example:\n",
    "    * Learning algorithm — takes a DataFrame to fit (training set) and gives ML model (Transfomer) \n",
    "\n",
    "* **Pipeline** — chain of trasformers and estimators. Pipeline represents ML workflow\n",
    "\n",
    "* **Parameter** — API to share parameters in Pipeline\n",
    "\n",
    "Pipeline is a sequence of stages, either Transformer or Estimator. When _fit()_ method called on a pipeline:\n",
    "    * For each Transormer method _transform()_ called on Dataframe.\n",
    "    * On Estimator stages _fit()_ called to produce Transformer. If there are more than one estimator, calls _transorm()_ on this transformer also.\n",
    "\n",
    "##### Example Pipeline\n",
    "before fit:\n",
    "![before fit](https://spark.apache.org/docs/latest/img/ml-Pipeline.png)\n",
    "\n",
    "1. Tokenizer — splits text into words\n",
    "2. HashingTF — converts words to feature vectors (adding new column to Dataframe)\n",
    "3. Logistic regression — classifies input vector\n",
    "\n",
    "after calling _fit()_ on Pipeline it becomes _PipelineModel_ (Transformer)\n",
    "\n",
    "after fit:\n",
    "![after fit](https://spark.apache.org/docs/latest/img/ml-PipelineModel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets write an example pipeline :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "// Prepare training documents from a list of (id, text, label) tuples.\n",
    "val training = spark.createDataFrame(Seq(\n",
    "  (0L, \"a b c d e spark\", 1.0),\n",
    "  (1L, \"b d\", 0.0),\n",
    "  (2L, \"spark f g h\", 1.0),\n",
    "  (3L, \"hadoop mapreduce\", 0.0)\n",
    ")).toDF(\"id\", \"text\", \"label\")\n",
    "\n",
    "// Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "val tokenizer = new Tokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n",
    "\n",
    "val hashingTF = new HashingTF().setNumFeatures(1000).setInputCol(tokenizer.getOutputCol).setOutputCol(\"features\")\n",
    "\n",
    "val lr = new LogisticRegression().setMaxIter(10).setRegParam(0.001)\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(tokenizer, hashingTF, lr))\n",
    "\n",
    "// Fit the pipeline to training documents.\n",
    "val model = pipeline.fit(training)\n",
    "\n",
    "// Now we can optionally save the fitted pipeline to disk\n",
    "model.write.overwrite().save(\"/tmp/spark-logistic-regression-model\")\n",
    "\n",
    "// We can also save this unfit pipeline to disk\n",
    "pipeline.write.overwrite().save(\"/tmp/unfit-lr-model\")\n",
    "\n",
    "// And load it back in during production\n",
    "val sameModel = PipelineModel.load(\"/tmp/spark-logistic-regression-model\")\n",
    "\n",
    "// Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "val test = spark.createDataFrame(Seq((4L, \"spark i j k\"),\n",
    "  (5L, \"l m n\"),\n",
    "  (6L, \"spark hadoop spark\"),\n",
    "  (7L, \"apache hadoop\")\n",
    ")).toDF(\"id\", \"text\")\n",
    "\n",
    "// Make predictions on test documents.\n",
    "model.transform(test).select(\"id\", \"text\", \"probability\", \"prediction\").collect().foreach {\n",
    "    case Row(id: Long, text: String, prob: Vector, prediction: Double) =>\n",
    "        println(s\"($id, $text) --> prob=$prob, prediction=$prediction\")\n",
    "  }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
